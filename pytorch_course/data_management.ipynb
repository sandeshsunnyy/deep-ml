{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37d1333",
   "metadata": {},
   "source": [
    "# Data Management in PyTorch\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Often times the data that is provided to us may not be in a form that can be directly used in classes like DataLoader. For example the image names may be generic, and the labels might be in some other file (like a .mat file). In such cases. It is crucial that we learn how to define our own Datasets and Dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e7ca2",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "### Defining our own Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class OxfordFlowersDataset(Dataset):\n",
    "\n",
    "  def __init__(self, root_dir):\n",
    "    self.root_dir = root_dir\n",
    "    self.img_dir = os.path.join(root_dir, 'images')\n",
    "\n",
    "    labels_matlab = scipy.io.loadmat(os.path.join(root_dir, 'imagelabels.mat'))\n",
    "\n",
    "    self.labels = labels_matlab['labels'][0] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b3187",
   "metadata": {},
   "source": [
    "Here the method adopted is called Lazy loading of Data, because if we initialize the class with data as it is, it uses up alot of RAM, which is unnecessary. Instead, we just mention where to find the data.\n",
    "\n",
    "labels are adjusted by subtracting 1 because PyTorch expects that the class labels start from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22703e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "  return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c8460",
   "metadata": {},
   "source": [
    "Used for returning the total number of samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "\n",
    "  img_name = f'image_{idx+1:05d}.jpg'\n",
    "  img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "  image = Image.open(img_path)\n",
    "  label = self.labels[idx]\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438de586",
   "metadata": {},
   "source": [
    "This dunder function is used to return the image and its corresponding label for the index provided. img_name depends on the actual data in the directory. This only works for the image pattern in the actual dataset. \n",
    "\n",
    "Also here idx is incremented by 1 because the dataset images start with image_00001. If 1 was not added, it would have taken image number 00000 (or image_00000) which does not exist. \n",
    "\n",
    "So study the data, especially its metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22cc06",
   "metadata": {},
   "source": [
    "## Transform Pipelines (Quality)\n",
    "\n",
    "### Learning why raw data won't work\n",
    "\n",
    "Batching won't work because pytorch expects that the items in a batch are of same dimensions. Which is rarely the case for image data. Also, PyTorch expects tensors, not image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec8f5f",
   "metadata": {},
   "source": [
    "transforms.Resize(256) resizes the shorter edge to 256 whilst preserving the aspect ratio of the image. Hard resizing where we give both dimensions (256, 256) would distort the image. \n",
    "\n",
    "Then transforms.CenterCrop(224) is used to obtain the middle portion (the 224x224 square image) of the image.\n",
    "\n",
    "Now to convert the images into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e86479",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),        <------- Add this\n",
    "  transforms.Normalize(mean= [...],\n",
    "                        std= [...])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72690e0e",
   "metadata": {},
   "source": [
    "ToTensor() is called 'The tensor Bridge'. Before the bridge the data type is image. After the bridge, the data is tensor. So applying transforms that could only be applied to tensors to images would cause errors. So handle that properly.\n",
    "\n",
    "Adding transforms to the OxfordFlowersDataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordFlowersDataset(Dataset):\n",
    "\n",
    "  def __init__(self, root_dir, transform = None):\n",
    "    # all other code\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # all other code\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd79f5c",
   "metadata": {},
   "source": [
    "Now it could be batched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a28350",
   "metadata": {},
   "source": [
    "For debugging, Take single datapoints and apply transforms individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6fb703",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74481f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "  dataset, [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdea11e",
   "metadata": {},
   "source": [
    "This gives a good mix the entire data and distributes them according to the sizes mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71d36e",
   "metadata": {},
   "source": [
    "### Batching using DataLoader\n",
    "\n",
    "iterating through the dataloader object gives us batch-wise data.\n",
    "For iterating through the first batch without starting a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1a8db",
   "metadata": {},
   "source": [
    "## Bug-proofing\n",
    "\n",
    "### On-the-fly transformation of PyTorch\n",
    "\n",
    "Random transforms are applied to the training dataset as it is loaded, without extra memory usages, so that the model see different versions of the same image each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "  #Random augmentation transforms\n",
    "  transforms.RandomHorizontalFlip(p=0.5)\n",
    "  transforms.RandomRotation(degrees=10),\n",
    "  transforms.ColorJitter(brightness=0.2),\n",
    "\n",
    "  #Other preprocessing steps\n",
    "  transforms.Resize(256),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean= [...],\n",
    "                        std= [...])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a5d0f",
   "metadata": {},
   "source": [
    "\n",
    "### Corrupted files (Gracefully handling)\n",
    "\n",
    "In __getitem__ function include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b29b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.verify()\n",
    "image = Image.open(img_path)      #<---- Reopen the image, because verify, closes the file.\n",
    "\n",
    "if image.size[0] < 32 or image.size[1] < 32:\n",
    "  raise ValueError(f\"Image too small\")\n",
    "\n",
    "if image.mode != 'RGB':      #<---- Converting to RGB\n",
    "  image = image.convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0e6c0",
   "metadata": {},
   "source": [
    "In case of other Exceptions, take the next idx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_idx = (idx + 1) % len(self)\n",
    "return self.__getitem__(next_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77af3a",
   "metadata": {},
   "source": [
    "### Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "  import time\n",
    "  start_time = time.time()\n",
    "\n",
    "  self.access_counts[idx] = self.access_counts.get(idx, 0) + 1\n",
    "\n",
    "  result = super().__getitem__(idx)\n",
    "\n",
    "  load_time = time.time() - start_time\n",
    "  self.load_times.append(load_time)\n",
    "\n",
    "  if load_time > 1.0:\n",
    "    print(f\" Slow load for image index : {idx}\"\n",
    "          \"Time taken: {load_time:.2f}s\")\n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
