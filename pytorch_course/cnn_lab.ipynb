{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407004b3",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "We don't have to build the model around the entire classes. We can select a small subset of classes to start with and then slowly scale after checking performance. \n",
    "\n",
    "# CNN: Key Layers\n",
    "\n",
    "## Convolutional Layer (nn.Conv2d)\n",
    "\n",
    "This is the core building block of a CNN, using learnable filters to scan the image for visual features. The output is a set of \"feature maps\" that highlight where in the image these patterns appear.\n",
    "\n",
    "- in_channels: The number of channels from the previous layer; for the first layer, this is 3 for the RGB color channels.\n",
    "- out_channels: The number of filters the layer will learn, determining the number of output feature maps.\n",
    "- kernel_size: The dimensions of the filter, such as a 3x3 grid that examines a pixel and its immediate neighbors.\n",
    "- padding: Adds a border around the image, allowing the kernel to process edge pixels while preserving the image's dimensions.\n",
    "\n",
    "## ReLU Activation Function (nn.ReLU)\n",
    "\n",
    "An activation function that introduces non-linearity by changing all negative values in the feature maps to zero. This helps the model learn more complex patterns.\n",
    "\n",
    "## Max Pooling Layer (nn.MaxPool2d)\n",
    "\n",
    "This layer downsamples the feature maps by reducing their height and width, which makes the network more efficient. It slides a window over the feature map and keeps only the single largest value from that window, discarding the rest.\n",
    "\n",
    "- kernel_size: The size of the window to perform pooling on, such as a 2x2 area.\n",
    "- stride: The step size the window moves across the image. A stride of 2 with a 2x2 kernel will halve the feature map's dimensions.\n",
    "\n",
    "## Flatten Layer (nn.Flatten)\n",
    "\n",
    "A utility layer that unrolls the 2D feature maps into a single 1D vector. This is a necessary step to prepare the data for the fully connected linear layers.\n",
    "\n",
    "## Linear Layer (nn.Linear)\n",
    "\n",
    "Also known as a fully connected layer, it performs the final classification. It combines the features learned by the convolutional layers into a final prediction.\n",
    "\n",
    "## Dropout Layer (nn.Dropout)\n",
    "\n",
    "A regularization technique that helps prevent overfitting by randomly setting a fraction of neuron activations to zero during training. This forces the network to learn more robust features instead of relying too heavily on any single pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5df09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_output_classes: int):\n",
    "        \n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_of_output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d845c",
   "metadata": {},
   "source": [
    "# Initialize Loss Function and Optimizer\n",
    "\n",
    "We'll use **nn.CrossEntropyLoss**. This is the standard loss function for multi-class classification tasks as it's designed to measure the error when a model has to choose one class from several possibilities.\n",
    "\n",
    "We'll use the **Adam** optimizer. This is a popular and efficient algorithm that updates the model's weights to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# prototype_model is the object of class SimpleCNN we create.\n",
    "optimizer_prototype = optim.Adam(prototype_model.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
