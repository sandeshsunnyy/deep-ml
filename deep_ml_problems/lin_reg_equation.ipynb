{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c19221",
   "metadata": {},
   "source": [
    "In a perfect world, your model would predict $y$ exactly:$$X\\beta = y$$\n",
    "\n",
    "However, in reality, this system is overdetermined (more equations/data points than variables). The columns of matrix $X$ span a subspace (a flat sheet or volume) inside a much larger space. This is called the Column Space ($C(X)$).\n",
    "\n",
    "The target vector $y$ almost never lies perfectly on this flat sheet. It sticks out into the empty space above it. Because $y$ is not on the sheet, no combination of $X$ can ever equal $y$.\n",
    "\n",
    "### The Geometric Solution: Projection\n",
    "\n",
    "Since we cannot reach $y$ exactly, we look for the closest possible point to $y$ that does exist on the sheet (the Column Space).\n",
    "Call this closest point $\\hat{y}$ (the prediction).\n",
    "\n",
    "$$\\hat{y} = X\\hat{\\beta}$$\n",
    "\n",
    "The shortest distance between a point ($y$) and a plane ($C(X)$) is the perpendicular (orthogonal) line. If you were standing on the plane directly underneath the floating point $y$, looking straight up, that spot you are standing on is $\\hat{y}$. This is the Orthogonal Projection of $y$ onto the column space of $X$.\n",
    "\n",
    "### Deriving the Equation from Geometry\n",
    "\n",
    "We can derive the Normal Equation purely using this geometric insight, without needing calculus derivatives.\n",
    "\n",
    "#### Step 1: Define the Error Vector\n",
    "\n",
    "The error (or residual) is the vector connecting our prediction to the truth: \n",
    "$$e = y - X\\hat{\\beta}$$\n",
    "\n",
    "#### Step 2: The Orthogonality Condition\n",
    "\n",
    "For the distance to be minimized, the error vector $e$ must be perpendicular (orthogonal) to the flat sheet (Column Space of $X$). If $e$ is perpendicular to the whole sheet, it must be perpendicular to every single column vector inside $X$. In linear algebra, two vectors are orthogonal if their dot product is zero. Therefore, the dot product of $X$ and the error $e$ is zero:\n",
    "\n",
    "$$X^T \\cdot e = 0$$\n",
    "\n",
    "#### Step 3: Solve for $\\beta$\n",
    "\n",
    "Substitute the definition of $e$ ($y - X\\hat{\\beta}$) into that equation: \n",
    "\n",
    "$$X^T (y - X\\hat{\\beta}) = 0$$\n",
    "\n",
    "Expand the terms:\n",
    "$$X^T y - X^T X \\hat{\\beta} = 0$$\n",
    "\n",
    "Move the negative term to the other side:\n",
    "$$X^T X \\hat{\\beta} = X^T y$$\n",
    "\n",
    "This is the Normal Equation!\n",
    "\n",
    "To isolate $\\hat{\\beta}$, strictly mathematically (assuming $X^T X$ is invertible), we multiply by the inverse:$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a08e5",
   "metadata": {},
   "source": [
    "## Analogy to understand better:\n",
    "\n",
    "In vector space, we have many 'observations'. Each row in X represents an observation. There can be many factors within each observation. For example, distance, driver_age etc. could be factors of a single observation. And y would represent what the actual values are for a dependent variable like time. \n",
    "\n",
    "In vector space, we take variable vectors, i.e, the column of X and y as vectors in the n-D vector space. While plotting these 'variable vectors' in vector space, the vectors defined by X lies in a different sub-space than y. But by using a special variable $\\beta$ we stretch the variable vectors to reach a specific point ($\\hat{y}$) such that the difference between the actual value ($y$) and this point is orthogonal to the entire subspace of $X$.\n",
    "\n",
    "Look at the example Gemini gave:\n",
    "\n",
    "- Variable Vectors ($X$): These define the \"Floor\" (The Subspace). You can reach any point on this floor by changing $\\beta$.\n",
    "- The Target ($y$): This is a balloon floating above the floor.\n",
    "- The Prediction ($\\hat{y}$): This is the shadow of the balloon on the floor.\n",
    "- The Error ($e$): This is the string connecting the balloon ($y$) to its shadow ($\\hat{y}$).\n",
    "\n",
    "Or another example:\n",
    "\n",
    "- $X$ is the Train Track: It is a line that goes on forever in two directions. You are the train driver. You can stop the train anywhere on this track (this is choosing your $\\beta$).\n",
    "- $y$ is the House: The house is at a specific, fixed location in the field. It is not a line. It is a single point (the tip of the vector).\n",
    "- The Problem: The house is not on the tracks.\n",
    "- The Solution (Regression): You drive the train along the infinite track ($X$) and stop at the exact spot that is closest to the house ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def linear_regression_normal_equation(X, y) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Solve linear regression via the normal equation using PyTorch.\n",
    "    X: Tensor or convertible of shape (m,n); y: shape (m,) or (m,1).\n",
    "    Returns a 1-D tensor of length n, rounded to 4 decimals.\n",
    "    \"\"\"\n",
    "    X_t = torch.as_tensor(X, dtype=torch.float)\n",
    "    y_t = torch.as_tensor(y, dtype=torch.float).reshape(-1,1)\n",
    "    # Your implementation here\n",
    "\n",
    "    first_transform = (X_t.T @ X_t)\n",
    "\n",
    "    final = torch.linalg.inv(first_transform) @ X_t.T @ y_t\n",
    "    return torch.round(final, decimals=4).squeeze()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
